{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cdade97-1b46-4f16-8caf-ffc88512753c",
   "metadata": {},
   "source": [
    "## Caching in Attention Models\n",
    "This challenge is about applying caching in attention models to speed up inference. We will use the pix2Struct model.\n",
    "</br>\n",
    "First, we will be exporting the checkpoint from HF using the right architecture, </br> </br>\n",
    "Note: run with python3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793bc1ac-cdcb-4d91-9227-602668c952e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install optimum\n",
    "!optimum-cli export onnx --model=\"google/pix2struct-docvqa-base\" \\\n",
    "    --device \"cpu\" --atol=1e-3 --framework=\"pt\" \\\n",
    "    --task=\"visual-question-answering-with-past\" \\\n",
    "    \"./export/original/docvqa/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cdc87-b5a8-45c6-be5f-3a783e970b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./export/original/docvqa/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e7b3f-512f-4a11-997b-aa1c535af8d6",
   "metadata": {},
   "source": [
    "## Sample Inference\n",
    "Now we will run some inference in the plain model - encoder_model.onnx, and decoder_model.onnx</br>\n",
    "You will re-use all these image pre-processing routines, tokenization, question-on-top-of-image rendering, etc. </br>\n",
    "You need to focus only in \"wiring\" model inputs/outputs to obtain the desired speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181480ff-178d-4394-bb75-e9b2e66c64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from inference import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b0eec-6801-464f-8a7b-9f0b0339588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines the questions\n",
    "questions = [\"What happens from 11:44am to 12:25am?\",\n",
    "             \"What is the designated time for Questions and Answers?\",\n",
    "             \"When is the Coffee Break?\",\n",
    "             \"Who is giving the Introductory Remarks?\",\n",
    "             \"Who is going to take part of the individual interviews?\",\n",
    "             \"What time do the Exhibits Open?\",\n",
    "             \"Where will the Coffee be served?\",\n",
    "             \"Who is the TRRF Vice President?\",\n",
    "             \"What is the designated time for TRRF Scientific Advisory Council Meeting?\",\n",
    "             \"Who is the TRRF Treasurer?\"             \n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fba7e-0821-4ff5-92df-7eee17bd41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare inputs for the run wrapper present in Inference script\n",
    "decoderModelPath = \"./export/original/docvqa/decoder_model.onnx\"\n",
    "encoderModelPath = \"./export/original/docvqa/encoder_model.onnx\"\n",
    "\n",
    "inputs = {}\n",
    "inputs[\"encoderPath\"] = encoderModelPath\n",
    "inputs[\"decoderPath\"] = decoderModelPath\n",
    "inputs[\"decoderWithCachePath\"] = decoderModelPath\n",
    "inputs[\"pieceModelPath\"] = \"./export/original/docvqa/spiece.model\"\n",
    "inputs[\"fontPath\"] = \"./resources/Arial.ttf\"\n",
    "inputs[\"imagePath\"] = \"./resources/download.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b07ab-a3c2-4ada-a345-7672322239f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the sample image\n",
    "img = Image.open(inputs[\"imagePath\"])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c63de5e-b9ea-4cd3-92f9-682da2744ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform inference\n",
    "encoderTime = [] \n",
    "decoderTime = []\n",
    "originalAnswers = [] \n",
    "\n",
    "for question in questions:\n",
    "    temp_result = {}\n",
    "    ques,ans,ecoder_time,decoder_time,image_time = run(inputs,question,weightsType=32,cache=False,log=False)\n",
    "    temp_result[\"decoded_question\"] = ques\n",
    "    temp_result[\"decoded_answer\"] = ans\n",
    "    temp_result[\"encoder_time\"] = ecoder_time\n",
    "    temp_result[\"decoder_time\"] = decoder_time\n",
    "    \n",
    "    \n",
    "    encoderTime.append(ecoder_time)\n",
    "    decoderTime.append(decoder_time)\n",
    "    \n",
    "    cleanedAnswer =  re.sub(r'[^\\w]', '', ans).lower()\n",
    "    originalAnswers.append(cleanedAnswer)\n",
    "\n",
    "    print(temp_result,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a74da-e95d-4d82-9b11-9d32c9450fca",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "Now it's when the fun begins! You will make caching work in the decoder by using the decoder_model_merged.onnx you obtained on previous steps. Rules,</br>\n",
    "* Feel free to modify existing files to accomodate for the new caching feature.\n",
    "* Code quality matters.\n",
    "* Memory utilization matters.\n",
    "* Good selection of data structures, algorithmic complexity matters.\n",
    "* Documentation.... well you guessed it, it matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare inputs for the run wrapper present in Inference script -- keep almost same as above\n",
    "decoderModelPath = \"./export/original/docvqa/decoder_model.onnx\"\n",
    "decoderWithCacheModelPath = \"./export/original/docvqa/decoder_model_merged.onnx\"\n",
    "encoderModelPath = \"./export/original/docvqa/encoder_model.onnx\"\n",
    "\n",
    "inputs = {}\n",
    "inputs[\"encoderPath\"] = encoderModelPath\n",
    "inputs[\"decoderPath\"] = decoderModelPath\n",
    "inputs[\"decoderWithCachePath\"] = decoderWithCacheModelPath\n",
    "inputs[\"pieceModelPath\"] = \"./export/original/docvqa/spiece.model\"\n",
    "inputs[\"fontPath\"] = \"./resources/Arial.ttf\"\n",
    "inputs[\"imagePath\"] = \"./resources/download.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform inference -- keep the same interface as before\n",
    "encoderTime = [] \n",
    "decoderTime = []\n",
    "originalAnswers = [] \n",
    "\n",
    "for question in questions:\n",
    "    temp_result = {}\n",
    "    # Just change the cache to True\n",
    "    ques,ans,ecoder_time,decoder_time,image_time = run(inputs,question,weightsType=32,cache=True,log=False)\n",
    "    temp_result[\"decoded_question\"] = ques\n",
    "    temp_result[\"decoded_answer\"] = ans\n",
    "    temp_result[\"encoder_time\"] = ecoder_time\n",
    "    temp_result[\"decoder_time\"] = decoder_time \n",
    "    \n",
    "    encoderTime.append(ecoder_time)\n",
    "    decoderTime.append(decoder_time)\n",
    "    \n",
    "    cleanedAnswer =  re.sub(r'[^\\w]', '', ans).lower()\n",
    "    originalAnswers.append(cleanedAnswer)\n",
    "\n",
    "    print(temp_result,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "print(\"Decoder Merged\")\n",
    "model = onnx.load(\"export/original/docvqa/decoder_model_merged.onnx\")\n",
    "for _input in model.graph.input:\n",
    "    print(MessageToDict(_input))\n",
    "print(\"=====================================\")\n",
    "print(\"=====================================\")\n",
    "print(\"Decoder\")\n",
    "model = onnx.load(\"export/original/docvqa/decoder_model.onnx\")\n",
    "for _input in model.graph.input:\n",
    "    print(MessageToDict(_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import onnx\n",
    "\n",
    "options = onnxruntime.SessionOptions()\n",
    "#options.log_severity_level = 0\n",
    "options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "#options.enable_mem_pattern = True\n",
    "\n",
    "#coso = onnxruntime.InferenceSession(\"./export/original/docvqa/decoder_model_merged.onnx\")\n",
    "\n",
    "model = onnx.load(\"./export/original/docvqa/decoder_model_merged.onnx\")\n",
    "coso = onnxruntime.InferenceSession(model.SerializeToString(), options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
