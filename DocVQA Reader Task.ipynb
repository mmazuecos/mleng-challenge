{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b305a68-4399-42cf-9411-ffb4a5ca87df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Task Description\n",
    "You're gonna create a dataset reader for the Visual Document Question Answering task.\n",
    "+ Get the dataset from this [link](https://rrc.cvc.uab.es/?ch=17&com=downloads)\n",
    "+ You must support reading the train and test datasets.\n",
    "+ You must support the super simple interface readDataset()\n",
    "+ The schema should be as provided in the cell below(check details)\n",
    "+ Provide a github repo, and setup instructions.\n",
    "+ I will test this in a cluster, so it must serialize things properly(from one executor to another, from one node to another).\n",
    "+ Write the solution in Scala, with Python wrappers, so that it can be called this way, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/10 17:43:37 WARN Utils: Your hostname, Sheoldred.local resolves to a loopback address: 127.0.0.1; using 192.168.100.214 instead (on interface en0)\n",
      "23/12/10 17:43:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/12/10 17:43:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/10 17:43:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"scala_pyspark\") \\\n",
    ".config(\"spark.jars\", \"./docvqareader_2.12-0.1.jar\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5b382e7-26b8-4a41-afff-48aa294cf418",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from jsl.task.docvqa_reader import DocVQA\n",
    "\n",
    "# this is the path in which the .json file is located\n",
    "# path = \"data/DocVQA/train_v1.0_withQT.json\"\n",
    "path = \"data/DocVQA/test_v1.0.json\"\n",
    "df = DocVQA().readDataset(spark, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b142e949-df58-4245-be5d-f0c8a7a5756b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Schema\n",
    "This is what the schema of the returned Dataframe should look like. The idea is that each row will contain a number of questions, and each of these questions will have multiple answers, on the same row.\n",
    "+ path: path to the file, it can be on a DFS.\n",
    "+ modificationTime: this value comes from the OS.\n",
    "+ questions: an array of questions.\n",
    "+ answers: a 2D array with each inner level array providing a set of candidate answers for each question in that same row.\n",
    "+ a binary buffer containing the image(see 'content' field below).\n",
    "\n",
    "Check the dataset JSON schema in the dataset for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1161776-e0e4-43ae-a873-a3e86800992a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = false)\n",
      " |-- length: integer (nullable = false)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- questions: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- answers: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's count the number of questions\n",
    "You should support all these type of operations without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5474dec-3150-4a2e-920a-dc9c73c5eeb9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "questions = df.select(explode(\"questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e3c9e2-e810-4b8d-b608-fb6c9e2d8a84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39463"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ba88313-b70f-4301-8aae-a3a1cd76999b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DocVQA Reader Task",
   "notebookOrigID": 151655299090514,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
